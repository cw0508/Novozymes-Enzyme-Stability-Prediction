{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle logistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installations\n",
    "! pip install py3Dmol -q\n",
    "! pip install kaggle\n",
    "! pip install biopandas\n",
    "! pip install Bio\n",
    "\n",
    "# Set the working directory to Desktop/kaggle\n",
    "! mkdir -p ~/Desktop/kaggle\n",
    "%cd ~/Desktop/kaggle/\n",
    "\n",
    "# Copy kaggle.json from Desktop/kaggle to the correct location\n",
    "! cp kaggle.json ~/.kaggle/\n",
    "! chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "# Download competition files to the current directory (Desktop/kaggle)\n",
    "! kaggle competitions download -c novozymes-enzyme-stability-prediction\n",
    "\n",
    "# Unzip in the current directory\n",
    "! unzip -o novozymes-enzyme-stability-prediction.zip\n",
    "\n",
    "# Download additional files to the current directory\n",
    "! kaggle kernels output cdeotte/train-data-contains-mutations-like-test-data -p ./\n",
    "! kaggle datasets download -d cdeotte/nesp-kaggle-train-wild-types-pdb-files -p ./\n",
    "\n",
    "# Unzip the PDB files in current directory\n",
    "! unzip -o nesp-kaggle-train-wild-types-pdb-files.zip\n",
    "\n",
    "# List files to verify everything is in Desktop/kaggle\n",
    "! ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "#scipy\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "#Tensorflow\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Input, Concatenate, Dropout, BatchNormalization\n",
    "\n",
    "#xgboost\n",
    "import xgboost as xgb\n",
    "\n",
    "#Biopandas\n",
    "from biopandas.pdb import PandasPdb\n",
    "from Bio.PDB import PDBParser\n",
    "from Bio.PDB.SASA import ShrakeRupley\n",
    "\n",
    "#Py3Dmol\n",
    "import py3Dmol\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Py3Dmol to show the wildtype structure.\n",
    "pdb_path = os.path.expanduser('~/Desktop/kaggle/wildtype_structure_prediction_af2.pdb')\n",
    "\n",
    "with open(pdb_path) as ifile:\n",
    "    protein = \"\".join([x for x in ifile])\n",
    "\n",
    "view = py3Dmol.view(width=800, height=600)\n",
    "view.addModelsAsFrames(protein)\n",
    "style = {'cartoon': {'color': 'spectrum'}, 'stick':{}}\n",
    "view.setStyle({'model': -1}, style)\n",
    "view.zoom(0.12)\n",
    "view.rotate(235, {'x':0, 'y':1, 'z':1})\n",
    "view.spin({'x':-0.2, 'y':0.5, 'z':1}, 1)\n",
    "\n",
    "# Save as HTML file\n",
    "html_path = os.path.expanduser('~/Desktop/kaggle/protein_animation.html')\n",
    "view.write_html(html_path)\n",
    "print(f\"Animation saved as: {html_path}\")\n",
    "\n",
    "# Show in notebook\n",
    "view.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shrake-Rupley algorithm for calculating Solvent Accessible Surface Area (SASA).\n",
    "p = PDBParser(QUIET=1)\n",
    "struct = p.get_structure(\"wildtype_structure_prediction_af2\", \"wildtype_structure_prediction_af2.pdb\")\n",
    "sr = ShrakeRupley()\n",
    "sr.compute(struct, level=\"S\")\n",
    "print(\"Solvent Accessible Surface Area (SASA):\", struct.sasa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data.\n",
    "train_path = os.path.expanduser('~/Desktop/kaggle/train.csv')\n",
    "train = pd.read_csv(train_path)\n",
    "\n",
    "print(\"Columns in train data:\")\n",
    "print(train.columns.tolist())\n",
    "print(f\"\\nData shape: {train.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After checking the columns, use the appropriate one:\n",
    "# Option 1: analyze sequence lengths\n",
    "if 'protein_sequence' in train.columns:\n",
    "    train['seq_length'] = train['protein_sequence'].str.len()\n",
    "    vc = train['seq_length'].value_counts()\n",
    "    print(f\"There are {len(vc)} unique sequence lengths\")\n",
    "    print(vc.head())\n",
    "    \n",
    "    # Additional sequence length insights\n",
    "    print(f\"\\nSequence Length Statistics:\")\n",
    "    print(f\"Shortest sequence: {train['seq_length'].min()} amino acids\")\n",
    "    print(f\"Longest sequence: {train['seq_length'].max()} amino acids\")\n",
    "    print(f\"Average length: {train['seq_length'].mean():.1f} ± {train['seq_length'].std():.1f} amino acids\")\n",
    "    \n",
    "    # Most common lengths\n",
    "    print(f\"\\nTop 5 most common sequence lengths:\")\n",
    "    for length, count in vc.head().items():\n",
    "        print(f\"  Length {length}: {count} sequences ({count/len(train)*100:.1f}%)\")\n",
    "\n",
    "# Option 2: analyze sequence IDs\n",
    "if 'seq_id' in train.columns:\n",
    "    vc = train['seq_id'].value_counts()\n",
    "    print(f\"\\nThere are {len(vc)} unique sequence IDs\")\n",
    "    print(\"Sequence ID value counts (should all be 1 if unique):\")\n",
    "    print(vc.head())\n",
    "    \n",
    "    # Check if seq_id is actually unique\n",
    "    if vc.max() == 1:\n",
    "        print(\"✓ All sequence IDs are unique\")\n",
    "    else:\n",
    "        print(f\"⚠ Some sequence IDs appear multiple times (max count: {vc.max()})\")\n",
    "\n",
    "# Show all columns and their unique value counts\n",
    "print(f\"\\n=== COMPLETE DATASET ANALYSIS ===\")\n",
    "print(\"Available columns and their unique value counts:\")\n",
    "for col in train.columns:\n",
    "    unique_count = train[col].nunique()\n",
    "    data_type = train[col].dtype\n",
    "    missing_count = train[col].isnull().sum()\n",
    "    \n",
    "    print(f\"  - {col}: {unique_count} unique values ({data_type}), {missing_count} missing\")\n",
    "    \n",
    "    # Show some examples for non-numeric columns\n",
    "    if data_type == 'object' and col != 'protein_sequence':\n",
    "        print(f\"    Sample values: {train[col].unique()[:3]}\")\n",
    "\n",
    "# Additional analysis for key columns\n",
    "print(f\"\\n=== KEY METRICS ===\")\n",
    "if 'tm' in train.columns:\n",
    "    print(f\"Target variable (tm) - Melting Temperature:\")\n",
    "    print(f\"  Range: {train['tm'].min():.1f}°C to {train['tm'].max():.1f}°C\")\n",
    "    print(f\"  Mean: {train['tm'].mean():.1f}°C ± {train['tm'].std():.1f}°C\")\n",
    "    print(f\"  Median: {train['tm'].median():.1f}°C\")\n",
    "\n",
    "if 'pH' in train.columns:\n",
    "    print(f\"\\npH conditions:\")\n",
    "    print(f\"  Range: {train['pH'].min()} to {train['pH'].max()}\")\n",
    "    print(f\"  Unique values: {sorted(train['pH'].unique())}\")\n",
    "\n",
    "if 'data_source' in train.columns:\n",
    "    print(f\"\\nData sources:\")\n",
    "    source_counts = train['data_source'].value_counts()\n",
    "    for source, count in source_counts.items():\n",
    "        print(f\"  {source}: {count} sequences ({count/len(train)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below implements the Levenshtein distance algorithm (also called edit distance)\n",
    "def leven(a, b):\n",
    "    '''\n",
    "    It calculates the minimum number of single-character edits (insertions, deletions, substitutions) needed to change one string into another. \n",
    "    This group similar protein sequences based on their evolutionary relationships.\n",
    "    '''\n",
    "    # we must add an additional character at the start of each string\n",
    "    a = f' {a}'\n",
    "    b = f' {b}'\n",
    "    # initialize empty zero array\n",
    "    lev = np.zeros((len(a), len(b)))\n",
    "    # now begin iterating through each value, finding the best path\n",
    "    for i in range(len(a)):\n",
    "        for j in range(len(b)):\n",
    "            if min([i, j]) == 0:\n",
    "                lev[i, j] = max([i, j])\n",
    "            else:\n",
    "                # calculate our three possible operations\n",
    "                x = lev[i-1, j]  # deletion\n",
    "                y = lev[i, j-1]  # insertion\n",
    "                z = lev[i-1, j-1]  # substitution\n",
    "                # take the minimum (eg best path/operation)\n",
    "                lev[i, j] = min([x, y, z])\n",
    "                # and if our two current characters don't match, add 1\n",
    "                if a[i] != b[j]:\n",
    "                    # if we have a match, don't add 1\n",
    "                    lev[i, j] += 1\n",
    "    return lev, lev[-1, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for sequences with length 231\n",
    "train231 = train[train.seq_length == 231].reset_index()\n",
    "train231"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Levenshtein distance matrix to compare all sequences of length 231 against each other\n",
    "!pip install rapidfuzz -q\n",
    "from rapidfuzz import distance\n",
    "\n",
    "sequences = train231[\"protein_sequence\"].values\n",
    "n = len(sequences)\n",
    "matrix = np.zeros((n, n))\n",
    "\n",
    "# Process in chunks to avoid memory issues\n",
    "chunk_size = 50\n",
    "for i in range(0, n, chunk_size):\n",
    "    for j in range(i, n, chunk_size):\n",
    "        for idx_i in range(i, min(i+chunk_size, n)):\n",
    "            for idx_j in range(j, min(j+chunk_size, n)):\n",
    "                if idx_i == idx_j:\n",
    "                    matrix[idx_i][idx_j] = 0\n",
    "                else:\n",
    "                    matrix[idx_i][idx_j] = distance.Levenshtein.distance(\n",
    "                        sequences[idx_i], sequences[idx_j]\n",
    "                    )\n",
    "                    matrix[idx_j][idx_i] = matrix[idx_i][idx_j]\n",
    "    \n",
    "    print(f\"Progress: {min(i+chunk_size, n)}/{n}\")\n",
    "\n",
    "print(\"Matrix completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appending B-factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "train_path = os.path.expanduser('~/Desktop/kaggle/train.csv')\n",
    "pdb_train = pd.read_csv(train_path)\n",
    "\n",
    "# Initialize columns\n",
    "pdb_train[\"b_factor\"] = 1.00\n",
    "pdb_train[\"SASA\"] = 3.00\n",
    "\n",
    "p = PDBParser(QUIET=1)\n",
    "sr = ShrakeRupley()\n",
    "kaggle_dir = os.path.expanduser('~/Desktop/kaggle')\n",
    "\n",
    "print(\"Available columns in pdb_train:\", pdb_train.columns.tolist())\n",
    "print(\"Available PDB files:\", len([f for f in os.listdir(kaggle_dir) if f.endswith('.pdb')]))\n",
    "\n",
    "# Use wildtype for all sequences (since no CIF column for individual PDB mapping)\n",
    "wildtype_pdb = \"wildtype_structure_prediction_af2.pdb\"\n",
    "wildtype_path = os.path.join(kaggle_dir, wildtype_pdb)\n",
    "\n",
    "if os.path.exists(wildtype_path):\n",
    "    try:\n",
    "        # Calculate wildtype values once\n",
    "        wildtype_b_factor = PandasPdb().read_pdb(wildtype_path).df['ATOM'].groupby('residue_number').b_factor.agg('first').mean()\n",
    "        wildtype_struct = p.get_structure(wildtype_pdb, wildtype_path)\n",
    "        sr.compute(wildtype_struct, level=\"S\")\n",
    "        wildtype_sasa = wildtype_struct.sasa\n",
    "        \n",
    "        # Apply to all rows\n",
    "        pdb_train[\"b_factor\"] = wildtype_b_factor\n",
    "        pdb_train[\"SASA\"] = wildtype_sasa\n",
    "        \n",
    "        print(f\"Applied wildtype values to all sequences: b_factor={wildtype_b_factor:.2f}, SASA={wildtype_sasa:.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing wildtype: {e}\")\n",
    "\n",
    "# Filter (this include all rows since we used wildtype for all)\n",
    "pdb_train_filtered = pdb_train[pdb_train.b_factor.isna() == False]\n",
    "\n",
    "# No need to reassign protein_sequence since mutant_seq doesn't exist\n",
    "print(\"No 'mutant_seq' column found, using existing 'protein_sequence'\")\n",
    "print(f\"Filtered data shape: {pdb_train_filtered.shape}\")\n",
    "\n",
    "# Drop columns that might not exist\n",
    "columns_to_drop = [\"dTm\", \"MUT\", \"position\", \"WT\", \"PDB\", \"CIF\", \"sequence\", \"mutant_seq\"]\n",
    "existing_columns_to_drop = [col for col in columns_to_drop if col in pdb_train_filtered.columns]\n",
    "\n",
    "if existing_columns_to_drop:\n",
    "    pdb_dropped = pdb_train_filtered.drop(existing_columns_to_drop, axis=1)\n",
    "else:\n",
    "    pdb_dropped = pdb_train_filtered.copy()\n",
    "\n",
    "print(f\"After dropping columns: {pdb_dropped.shape}\")\n",
    "\n",
    "# NEW: Load grouped data and merge with PDB calculations\n",
    "try:\n",
    "    grouped_path = os.path.expanduser('~/Desktop/kaggle/train_with_groups.csv')\n",
    "    grouped = pd.read_csv(grouped_path)\n",
    "    print(f\"Loaded grouped data: {grouped.shape}\")\n",
    "    \n",
    "    # Find common sequences between grouped data and our PDB-calculated data\n",
    "    common_sequences = set(pdb_dropped['protein_sequence']).intersection(set(grouped['protein_sequence']))\n",
    "    print(f\"Found {len(common_sequences)} common sequences between datasets\")\n",
    "    \n",
    "    # Filter grouped data to only include sequences we have PDB calculations for\n",
    "    filter2 = grouped[grouped['protein_sequence'].isin(common_sequences)].copy()\n",
    "    filter2 = filter2[filter2[\"data_source\"].isna() == False]\n",
    "    filter2 = filter2.drop_duplicates(subset=['seq_id']).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Filtered grouped data: {filter2.shape}\")\n",
    "    \n",
    "    # Initialize b_factor and SASA in filter2\n",
    "    filter2[\"b_factor\"] = 1.000\n",
    "    filter2[\"SASA\"] = 3.000\n",
    "    \n",
    "    # Transfer PDB calculations from pdb_dropped to filter2\n",
    "    for row in range(len(filter2)):\n",
    "        seq = filter2.iloc[row][\"protein_sequence\"]\n",
    "        matching_rows = pdb_dropped[pdb_dropped[\"protein_sequence\"] == seq]\n",
    "        \n",
    "        if not matching_rows.empty:\n",
    "            filter2.loc[filter2.index[row], \"b_factor\"] = matching_rows[\"b_factor\"].mean()\n",
    "            filter2.loc[filter2.index[row], \"SASA\"] = matching_rows[\"SASA\"].mean()\n",
    "    \n",
    "    print(\"Transferred PDB calculations to grouped data\")\n",
    "    print(f\"Final filter2 shape: {filter2.shape}\")\n",
    "    print(filter2[['seq_id', 'protein_sequence', 'b_factor', 'SASA']].head())\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Grouped data file not found, using pdb_dropped as final result\")\n",
    "    filter2 = pdb_dropped.copy()\n",
    "\n",
    "print(\"\\n=== FINAL RESULTS ===\")\n",
    "print(\"Available datasets:\")\n",
    "print(f\"- pdb_dropped: {pdb_dropped.shape} (original training data with PDB values)\")\n",
    "if 'filter2' in locals():\n",
    "    print(f\"- filter2: {filter2.shape} (grouped data with transferred PDB values)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE-preprocessing of TEST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading PDBs for the testing dataset\n",
    "!kaggle datasets download -d roberthatch/nesp-kvigly-test-mutation-pdbs\n",
    "\n",
    "# Unzipping PDBs for the testing dataset\n",
    "!unzip -o nesp-kvigly-test-mutation-pdbs.zip\n",
    "\n",
    "# Wild-type Amino acid sequence\n",
    "WT = \"VPVNPEPDATSVENVALKTGSGDSQSDPIKADLEVKGQSALPFDVDCWAILCKGAPNVLQRVNEKTKNSNRDRSGANKGPFKDPQKWGIKALPPKNPSWSAQDFKSPEEYAFASSLQGGTNAILAPVNLASQNSQGGVLNGFYSANKVAQFDPSKPQQTKGTWFQITKFTGAAGPYCKALGSNDKSVCDKNKNIAGDWGFDPAKWAYQYDEKNNKFNYVGK\"\n",
    "\n",
    "# Load test data with correct path\n",
    "test_path = os.path.expanduser('~/Desktop/kaggle/test.csv')\n",
    "test = pd.read_csv(test_path)\n",
    "\n",
    "print(f\"Test data shape: {test.shape}\")\n",
    "print(\"Test columns:\", test.columns.tolist())\n",
    "\n",
    "# Initialize lists and columns\n",
    "WT_mut = []\n",
    "mut_pos = []\n",
    "mutate = []\n",
    "\n",
    "test[\"pdb_pos\"] = \"temp\"\n",
    "\n",
    "# Fix 1: Use .loc for assignment to avoid warnings\n",
    "for index, seq in enumerate(test[\"protein_sequence\"]):\n",
    "    if len(seq) == len(WT):\n",
    "        mutation_found = False\n",
    "        for pos in range(len(seq)):\n",
    "            if seq == WT:\n",
    "                WT_mut.append(WT[pos])\n",
    "                mutate.append(seq[pos])\n",
    "                mut_pos.append(pos+1)\n",
    "                test.loc[index, \"pdb_pos\"] = \"no_mutation\"\n",
    "                break\n",
    "            elif seq[pos] != WT[pos]:\n",
    "                WT_mut.append(WT[pos])\n",
    "                mutate.append(seq[pos])\n",
    "                mut_pos.append(pos+1)\n",
    "                test.loc[index, \"pdb_pos\"] = f\"{WT[pos]}{pos+1}{seq[pos]}\"\n",
    "                mutation_found = True\n",
    "                break\n",
    "        if not mutation_found:\n",
    "            WT_mut.append(\"\")\n",
    "            mutate.append(\"\")\n",
    "            mut_pos.append(\"\")\n",
    "    else:\n",
    "        # Handle sequences of different lengths\n",
    "        mutation_found = False\n",
    "        for pos in range(min(len(seq), len(WT))):\n",
    "            if seq[pos] != WT[pos]:\n",
    "                WT_mut.append(WT[pos])\n",
    "                mut_pos.append(pos+1)\n",
    "                mutate.append(\"_\")\n",
    "                test.loc[index, \"pdb_pos\"] = f\"{WT[pos]}{pos+1}_\"\n",
    "                mutation_found = True\n",
    "                break\n",
    "        if not mutation_found:\n",
    "            WT_mut.append(\"\")\n",
    "            mut_pos.append(\"\")\n",
    "            mutate.append(\"\")\n",
    "\n",
    "# Initialize PDB parser\n",
    "p = PDBParser(QUIET=1)\n",
    "sr = ShrakeRupley()\n",
    "\n",
    "# Fix 2: Use full paths for PDB files\n",
    "kaggle_dir = os.path.expanduser('~/Desktop/kaggle')\n",
    "wildtype_pdb_path = os.path.join(kaggle_dir, \"wildtype_structure_prediction_af2.pdb\")\n",
    "\n",
    "# Calculate wildtype values\n",
    "try:\n",
    "    struct = p.get_structure(\"wildtype\", wildtype_pdb_path)\n",
    "    sr.compute(struct, level=\"S\")\n",
    "    wildtype_sasa = struct.sasa\n",
    "    wildtype_b_factor = PandasPdb().read_pdb(wildtype_pdb_path).df['ATOM'].b_factor.mean()\n",
    "    \n",
    "    # Initialize with wildtype values\n",
    "    test[\"b_factor\"] = wildtype_b_factor\n",
    "    test[\"SASA\"] = wildtype_sasa\n",
    "    \n",
    "    print(f\"Wildtype values - b_factor: {wildtype_b_factor:.2f}, SASA: {wildtype_sasa:.2f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error processing wildtype: {e}\")\n",
    "    # Fallback values\n",
    "    test[\"b_factor\"] = 1.0\n",
    "    test[\"SASA\"] = 3.0\n",
    "\n",
    "# Fix 3: Process mutation PDB files with proper error handling\n",
    "for row in range(len(test)):\n",
    "    if test.loc[row, \"pdb_pos\"] != \"temp\" and test.loc[row, \"pdb_pos\"] != \"no_mutation\":\n",
    "        pdb_filename = f\"{test.loc[row, 'pdb_pos']}_unrelaxed_rank_1_model_3.pdb\"\n",
    "        pdb_path = os.path.join(os.getcwd(), pdb_filename)  # Assuming PDBs are in current directory\n",
    "        \n",
    "        if os.path.exists(pdb_path):\n",
    "            try:\n",
    "                # Calculate b_factor for mutation\n",
    "                mutation_b_factor = PandasPdb().read_pdb(pdb_path).df['ATOM'].b_factor.mean()\n",
    "                test.loc[row, \"b_factor\"] = mutation_b_factor\n",
    "                \n",
    "                # Calculate SASA for mutation\n",
    "                struct = p.get_structure(pdb_filename, pdb_path)\n",
    "                sr.compute(struct, level=\"S\")\n",
    "                test.loc[row, \"SASA\"] = struct.sasa\n",
    "                \n",
    "                print(f\"Processed mutation {test.loc[row, 'pdb_pos']}: b_factor={mutation_b_factor:.2f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {pdb_filename}: {e}\")\n",
    "        else:\n",
    "            print(f\"PDB file not found: {pdb_path}\")\n",
    "\n",
    "# Fix 4: Download additional PDB files if needed\n",
    "# First check if we have train_pdb defined\n",
    "try:\n",
    "    if 'train_pdb' in locals() and hasattr(train_pdb, 'pdb_id'):\n",
    "        for pdb_id in train_pdb.pdb_id.unique():\n",
    "            if isinstance(pdb_id, str) and pdb_id not in ['', 'nan', None]:\n",
    "                pdb_file = f\"{pdb_id}.pdb\"\n",
    "                if not os.path.exists(pdb_file):\n",
    "                    os.system(f'wget https://files.rcsb.org/download/{pdb_id}.pdb')\n",
    "                    print(f\"Downloaded {pdb_id}.pdb\")\n",
    "                else:\n",
    "                    print(f\"{pdb_id}.pdb already exists\")\n",
    "    else:\n",
    "        print(\"train_pdb not defined or no pdb_id column\")\n",
    "except NameError:\n",
    "    print(\"train_pdb not defined\")\n",
    "\n",
    "print(\"Processing complete!\")\n",
    "print(f\"Final test data shape: {test.shape}\")\n",
    "print(test[['protein_sequence', 'pdb_pos', 'b_factor', 'SASA']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the files\n",
    "train_updates = pd.read_csv('train_updates_20220929.csv', index_col = 'seq_id')\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "grouped = pd.read_csv('train_with_groups.csv')\n",
    "\n",
    "# Groups of single mutation protein sequences\n",
    "grouped.group.value_counts()\n",
    "g5 = grouped[(grouped.pH == 7) & (grouped.group == 9)]\n",
    "g6 = grouped[(grouped.pH == 8) & (grouped.group == 34)]\n",
    "\n",
    "# Use the existing data\n",
    "df_train = filter2\n",
    "df_test = test\n",
    "\n",
    "# Apply training updates (only if data_source exists in train_updates)\n",
    "if 'data_source' in train_updates.columns:\n",
    "    train_updates2 = train_updates.drop('data_source', axis = 1)\n",
    "    if 'data_source' in df_train.columns:\n",
    "        df_train = df_train.drop('data_source', axis = 1)\n",
    "    df_train.loc[train_updates2.dropna().index] = train_updates2.dropna(axis = 0)\n",
    "\n",
    "# Remove instances with abnormal pH values\n",
    "df_train = df_train[df_train['pH']<=14].copy()\n",
    "df_train = df_train[0<=df_train['pH']].copy()\n",
    "\n",
    "# Wild-type Amino acid sequence\n",
    "WT = \"VPVNPEPDATSVENVALKTGSGDSQSDPIKADLEVKGQSALPFDVDCWAILCKGAPNVLQRVNEKTKNSNRDRSGANKGPFKDPQKWGIKALPPKNPSWSAQDFKSPEEYAFASSLQGGTNAILAPVNLASQNSQGGVLNGFYSANKVAQFDPSKPQQTKGTWFQITKFTGAAGPYCKALGSNDKSVCDKNKNIAGDWGFDPAKWAYQYDEKNNKFNYVGK\"\n",
    "\n",
    "# Drop columns if they exist\n",
    "if 'group' in df_train.columns:\n",
    "    df_train.drop(columns='group', inplace=True)\n",
    "\n",
    "# Impute missing pH values\n",
    "df_train['pH'].fillna(df_train['pH'].median(), inplace=True)\n",
    "\n",
    "# Drop sequence ID column if it exists\n",
    "if 'seq_id' in df_train.columns:\n",
    "    df_train.drop(columns='seq_id', inplace=True)\n",
    "\n",
    "# Processing TRAIN dataset\n",
    "\n",
    "# Sequence features\n",
    "df_train['letter_count'] = df_train['protein_sequence'].apply(lambda x: len(set(x)))\n",
    "df_train['seq_len'] = df_train['protein_sequence'].str.len()\n",
    "\n",
    "# Get amino acids from first sequence\n",
    "amino_acids = list(set(df_train[\"protein_sequence\"].iloc[0]))\n",
    "\n",
    "# Amino acid composition columns for TRAIN\n",
    "for a in amino_acids:\n",
    "    df_train[f\"{a}\"] = df_train['protein_sequence'].apply(lambda x: x.count(a) / len(x))\n",
    "\n",
    "# Amino acid groups\n",
    "hydrophobic = [\"A\", \"I\", \"L\", \"M\", \"F\", \"W\", \"Y\", \"V\"]\n",
    "negative = [\"D\", \"E\"]\n",
    "positive = [\"R\", \"H\", \"K\"]\n",
    "special = [\"C\", \"G\", \"P\"]\n",
    "polar = [\"S\", \"T\", \"N\", \"Q\"]\n",
    "\n",
    "# Group abundances for TRAIN (only use amino acids that exist in our data)\n",
    "df_train[\"hydrophobic\"] = df_train[[a for a in hydrophobic if a in amino_acids]].sum(axis=1)\n",
    "df_train[\"negative\"] = df_train[[a for a in negative if a in amino_acids]].sum(axis=1)\n",
    "df_train[\"positive\"] = df_train[[a for a in positive if a in amino_acids]].sum(axis=1)\n",
    "df_train[\"special\"] = df_train[[a for a in special if a in amino_acids]].sum(axis=1)\n",
    "df_train[\"polar\"] = df_train[[a for a in polar if a in amino_acids]].sum(axis=1)\n",
    "\n",
    "# Test Dataset processing - CREATE AMINO ACID COLUMNS FIRST\n",
    "if df_test is not None:\n",
    "    # Add sequence features to test data\n",
    "    df_test['seq_len'] = df_test['protein_sequence'].str.len()\n",
    "    \n",
    "    # Create amino acid composition columns for TEST\n",
    "    for a in amino_acids:\n",
    "        df_test[f\"{a}\"] = df_test['protein_sequence'].apply(lambda x: x.count(a) / len(x))\n",
    "    \n",
    "    # Now add group abundances for TEST\n",
    "    df_test[\"hydrophobic\"] = df_test[[a for a in hydrophobic if a in amino_acids]].sum(axis=1)\n",
    "    df_test[\"negative\"] = df_test[[a for a in negative if a in amino_acids]].sum(axis=1)\n",
    "    df_test[\"positive\"] = df_test[[a for a in positive if a in amino_acids]].sum(axis=1)\n",
    "    df_test[\"special\"] = df_test[[a for a in special if a in amino_acids]].sum(axis=1)\n",
    "    df_test[\"polar\"] = df_test[[a for a in polar if a in amino_acids]].sum(axis=1)\n",
    "\n",
    "# Filter data\n",
    "df_train = df_train[df_train.tm > 35]\n",
    "\n",
    "# Drop index column if it exists\n",
    "if 'index' in df_train.columns:\n",
    "    df_train.drop(columns=\"index\", inplace=True)\n",
    "\n",
    "# Optional: filter sequence lengths\n",
    "# df_train = df_train[(df_train.seq_len>100) & (df_train.seq_len<400)]\n",
    "\n",
    "# Prepare features and target\n",
    "columns_to_drop = ['tm','protein_sequence']\n",
    "columns_to_drop = [col for col in columns_to_drop if col in df_train.columns]\n",
    "X = df_train.drop(columns=columns_to_drop)\n",
    "y = pd.DataFrame(df_train['tm'])\n",
    "\n",
    "# Prepare test features\n",
    "if df_test is not None:\n",
    "    test_columns_to_drop = ['index', 'seq_id', 'pdb_pos', 'protein_sequence']\n",
    "    test_columns_to_drop = [col for col in test_columns_to_drop if col in df_test.columns]\n",
    "    Xt = df_test.drop(columns=test_columns_to_drop)\n",
    "else:\n",
    "    Xt = None\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "print(\"Preprocessing completed!\")\n",
    "print(f\"Training features: {X_train.shape}\")\n",
    "print(f\"Test features: {X_test.shape}\")\n",
    "if Xt is not None:\n",
    "    print(f\"Prediction features: {Xt.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure our data is properly formatted\n",
    "print(\"Data types and shapes:\")\n",
    "print(f\"X_train type: {type(X_train)}, shape: {X_train.shape}\")\n",
    "print(f\"y_train type: {type(y_train)}, shape: {y_train.shape}\")\n",
    "\n",
    "# Convert to numpy arrays if needed\n",
    "X_train_array = X_train.values if hasattr(X_train, 'values') else X_train\n",
    "y_train_array = y_train.values.ravel() if hasattr(y_train, 'values') else y_train\n",
    "\n",
    "print(f\"X_train_array shape: {X_train_array.shape}\")\n",
    "print(f\"y_train_array shape: {y_train_array.shape}\")\n",
    "\n",
    "# Check for any issues in the data\n",
    "print(f\"Any NaN in X_train: {np.isnan(X_train_array).any()}\")\n",
    "print(f\"Any NaN in y_train: {np.isnan(y_train_array).any()}\")\n",
    "print(f\"Any Inf in X_train: {np.isinf(X_train_array).any()}\")\n",
    "print(f\"Any Inf in y_train: {np.isinf(y_train_array).any()}\")\n",
    "\n",
    "# Now try training\n",
    "try:\n",
    "    model1 = xgb.XGBRegressor(\n",
    "        n_estimators=140, \n",
    "        max_depth=4,\n",
    "        random_state=42,\n",
    "        verbosity=1\n",
    "    )\n",
    "    model1.fit(X_train_array, y_train_array)\n",
    "    print(\"Model trained successfully!\")\n",
    "    \n",
    "    # Make predictions to test\n",
    "    y_pred = model1.predict(X_test.values if hasattr(X_test, 'values') else X_test)\n",
    "    print(f\"Predictions shape: {y_pred.shape}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Training failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions1 = model1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spearman R for seeing how good our predictions are\n",
    "spearmanr(y_test.values.flatten(), predictions1.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple matplotlib version\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test.values.flatten(), predictions1.flatten(), alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel('True tm')\n",
    "plt.ylabel('Predicted tm')\n",
    "plt.title('True vs Predicted Melting Temperature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intermolecular forces "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "df_test1 = pd.read_csv('test.csv', index_col='seq_id')\n",
    "print(f\"Original test data shape: {df_test1.shape}\")\n",
    "\n",
    "# Processing TEST dataset\n",
    "if 'data_source' in df_test1.columns:\n",
    "    df_test1.drop(columns='data_source', inplace=True)\n",
    "\n",
    "df_test1.reset_index(inplace=True)\n",
    "\n",
    "# Get amino acids from training data\n",
    "amino_acids = list(set(df_train[\"protein_sequence\"].iloc[0]))\n",
    "\n",
    "# Feature engineering for test data\n",
    "df_test1['letter_count'] = df_test1['protein_sequence'].apply(lambda x: len(set(x)))\n",
    "df_test1['seq_len'] = df_test1['protein_sequence'].str.len()\n",
    "\n",
    "# Create amino acid composition columns\n",
    "for a in amino_acids:\n",
    "    df_test1[f\"{a}\"] = df_test1['protein_sequence'].apply(lambda x: x.count(a) / len(x))\n",
    "\n",
    "# Amino acid groups\n",
    "hydrophobic = [\"A\", \"I\", \"L\", \"M\", \"F\", \"W\", \"Y\", \"V\"]\n",
    "negative = [\"D\", \"E\"]\n",
    "positive = [\"R\", \"H\", \"K\"]\n",
    "special = [\"C\", \"G\", \"P\"]\n",
    "polar = [\"S\", \"T\", \"N\", \"Q\"]\n",
    "\n",
    "# Calculate group abundances\n",
    "df_test1[\"hydrophobic\"] = df_test1[[a for a in hydrophobic if a in amino_acids]].sum(axis=1)\n",
    "df_test1[\"negative\"] = df_test1[[a for a in negative if a in amino_acids]].sum(axis=1)\n",
    "df_test1[\"positive\"] = df_test1[[a for a in positive if a in amino_acids]].sum(axis=1)\n",
    "df_test1[\"special\"] = df_test1[[a for a in special if a in amino_acids]].sum(axis=1)\n",
    "df_test1[\"polar\"] = df_test1[[a for a in polar if a in amino_acids]].sum(axis=1)\n",
    "\n",
    "# Add missing PDB features to test data\n",
    "print(\"Adding missing PDB features to test data...\")\n",
    "# Use the same wildtype values we used for training\n",
    "wildtype_b_factor = 92.42\n",
    "wildtype_sasa = 11816.24\n",
    "\n",
    "df_test1['b_factor'] = wildtype_b_factor\n",
    "df_test1['SASA'] = wildtype_sasa\n",
    "\n",
    "print(f\"Processed test data shape: {df_test1.shape}\")\n",
    "\n",
    "# Prepare features for prediction - MUST MATCH TRAINING FEATURES\n",
    "training_features = X_train.columns.tolist()\n",
    "print(f\"Training features ({len(training_features)}): {training_features}\")\n",
    "\n",
    "# Check feature alignment\n",
    "available_features = [col for col in training_features if col in df_test1.columns]\n",
    "missing_features = [col for col in training_features if col not in df_test1.columns]\n",
    "\n",
    "print(f\"Available features in test: {len(available_features)}\")\n",
    "print(f\"Missing features in test: {missing_features}\")\n",
    "\n",
    "# Create test feature matrix with EXACTLY the same features as training\n",
    "X_test_final = df_test1[training_features]  # This will now work since we added b_factor and SASA\n",
    "\n",
    "print(f\"Final test features shape: {X_test_final.shape}\")\n",
    "\n",
    "# Make predictions\n",
    "print(\"Making predictions...\")\n",
    "y_pred = model1.predict(X_test_final)\n",
    "print(f\"Predictions shape: {y_pred.shape}\")\n",
    "print(f\"Prediction range: {y_pred.min():.2f} - {y_pred.max():.2f}\")\n",
    "\n",
    "# Generate submission file\n",
    "submission_model = submission.copy()\n",
    "submission_model['tm'] = y_pred\n",
    "\n",
    "# Validate submission format\n",
    "print(f\"Submission shape: {submission_model.shape}\")\n",
    "print(\"Submission preview:\")\n",
    "print(submission_model.head())\n",
    "\n",
    "# Save submission\n",
    "submission_model.to_csv('submission.csv', index=False)\n",
    "print(\"Saved submission.csv\")\n",
    "\n",
    "# Check the predictions\n",
    "print(\"\\nPrediction statistics:\")\n",
    "print(submission_model['tm'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fair-esm -q\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import esm\n",
    "\n",
    "# Wildtype sequence\n",
    "wt = \"VPVNPEPDATSVENVALKTGSGDSQSDPIKADLEVKGQSALPFDVDCWAILCKGAPNVLQRVNEKTKNSNRDRSGANKGPFKDPQKWGIKALPPKNPSWSAQDFKSPEEYAFASSLQGGTNAILAPVNLASQNSQGGVLNGFYSANKVAQFDPSKPQQTKGTWFQITKFTGAAGPYCKALGSNDKSVCDKNKNIAGDWGFDPAKWAYQYDEKNNKFNYVGK\"\n",
    "\n",
    "# Load test data\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "\n",
    "# FIX: Remove the problematic line that modifies sequence 1169\n",
    "# test_df.loc[1169, 'protein_sequence'] = wt[:-1]+\"!\" # This is incorrect\n",
    "\n",
    "# Find mutation positions\n",
    "def find_edit_position(seq, wildtype=wt):\n",
    "    \"\"\"Find the position where sequence differs from wildtype\"\"\"\n",
    "    if len(seq) != len(wildtype):\n",
    "        return 0  # Handle length mismatches\n",
    "    for i in range(len(seq)):\n",
    "        if seq[i] != wildtype[i]:\n",
    "            return i\n",
    "    return 0  # No mutation found\n",
    "\n",
    "test_df['edit_idx'] = test_df['protein_sequence'].apply(find_edit_position)\n",
    "print(f\"Found mutations at positions: {test_df['edit_idx'].unique()}\")\n",
    "\n",
    "# Load ESM-2 model\n",
    "print(\"Loading ESM-2 model...\")\n",
    "model, alphabet = esm.pretrained.esm2_t12_35M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model.eval()  # disables dropout for deterministic results\n",
    "\n",
    "print(\"ESM-2 model loaded successfully!\")\n",
    "\n",
    "# Prepare wildtype data\n",
    "data = [(\"wildtype\", wt)]\n",
    "batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "\n",
    "# Extract contact maps from wildtype\n",
    "print(\"Extracting contact maps...\")\n",
    "with torch.no_grad():\n",
    "    results = model(batch_tokens, repr_layers=[12], return_contacts=True)\n",
    "\n",
    "# Get contact map and calculate contact sums\n",
    "contact_map = results[\"contacts\"][0].numpy()  # Convert to numpy\n",
    "ac_sum = np.sum(contact_map, axis=1)\n",
    "\n",
    "print(f\"Contact map shape: {contact_map.shape}\")\n",
    "print(f\"Contact sums range: {ac_sum.min():.3f} - {ac_sum.max():.3f}\")\n",
    "\n",
    "# Make predictions based on contact sums at mutation positions\n",
    "# FIX: Handle cases where edit_idx might be out of bounds\n",
    "def safe_predict(edit_idx, contact_sums):\n",
    "    if edit_idx < len(contact_sums):\n",
    "        return 1.0 / contact_sums[edit_idx] if contact_sums[edit_idx] > 0 else 1.0\n",
    "    else:\n",
    "        return 1.0  # Default value for out-of-bounds\n",
    "\n",
    "test_df['tm'] = test_df['edit_idx'].apply(lambda x: safe_predict(x, ac_sum))\n",
    "\n",
    "print(\"Prediction statistics:\")\n",
    "print(f\"tm range: {test_df['tm'].min():.3f} - {test_df['tm'].max():.3f}\")\n",
    "print(f\"tm mean: {test_df['tm'].mean():.3f}\")\n",
    "\n",
    "# Generate submission\n",
    "submission_esm = test_df[['seq_id', 'tm']].copy()\n",
    "submission_esm.to_csv('submission_esm.csv', index=False)\n",
    "\n",
    "print(\"ESM-2 submission preview:\")\n",
    "print(submission_esm.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pure chem b-factor and SASA and Thermo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import rankdata\n",
    "from biopandas.pdb import PandasPdb\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "# Load test data\n",
    "test = pd.read_csv('test.csv')\n",
    "kaggle_dir = os.path.expanduser('~/Desktop/kaggle')\n",
    "\n",
    "# Identify deletion mutations (shorter sequences)\n",
    "deletions = test.loc[test.protein_sequence.str.len() == 220, 'seq_id'].values\n",
    "print(f\"Found {len(deletions)} deletion mutations\")\n",
    "\n",
    "# Wildtype sequence\n",
    "base = 'VPVNPEPDATSVENVALKTGSGDSQSDPIKADLEVKGQSALPFDVDCWAILCKGAPNVLQRVNEKTKNSNRDRSGANKGPFKDPQKWGIKALPPKNPSWSAQDFKSPEEYAFASSLQGGTNAILAPVNLASQNSQGGVLNGFYSANKVAQFDPSKPQQTKGTWFQITKFTGAAGPYCKALGSNDKSVCDKNKNIAGDWGFDPAKWAYQYDEKNNKFNYVGK'\n",
    "\n",
    "def get_test_mutation(row):\n",
    "    \"\"\"Identify the mutation position and type\"\"\"\n",
    "    # Handle sequences of different lengths\n",
    "    min_len = min(len(row.protein_sequence), len(base))\n",
    "    \n",
    "    for i in range(min_len):\n",
    "        if row.protein_sequence[i] != base[i]:\n",
    "            row['wildtype'] = base[i]\n",
    "            row['mutation'] = row.protein_sequence[i]\n",
    "            row['position'] = i + 1\n",
    "            return row\n",
    "    \n",
    "    # If no mutation found in overlapping region, it's a length change\n",
    "    row['wildtype'] = 'X'\n",
    "    row['mutation'] = '_'\n",
    "    row['position'] = 1\n",
    "    return row\n",
    "\n",
    "test = test.apply(get_test_mutation, axis=1)\n",
    "test.loc[test.seq_id.isin(deletions), 'mutation'] = '_'\n",
    "\n",
    "print(\"Mutation analysis:\")\n",
    "print(test[['seq_id', 'wildtype', 'position', 'mutation']].head(10))\n",
    "\n",
    "# Load wildtype b-factors\n",
    "print(\"Loading wildtype structure...\")\n",
    "try:\n",
    "    # Try different possible wildtype PDB filenames\n",
    "    wildtype_files = [\n",
    "        'wildtype_structure_prediction_af2.pdb',\n",
    "        'WT_unrelaxed_rank_1_model_3.pdb',\n",
    "        'wildtype.pdb'\n",
    "    ]\n",
    "    \n",
    "    wt_pdb_file = None\n",
    "    for file in wildtype_files:\n",
    "        if os.path.exists(os.path.join(kaggle_dir, file)):\n",
    "            wt_pdb_file = os.path.join(kaggle_dir, file)\n",
    "            break\n",
    "    \n",
    "    if wt_pdb_file:\n",
    "        atom_df0 = PandasPdb().read_pdb(wt_pdb_file)\n",
    "        atom_df0 = atom_df0.df['ATOM']\n",
    "        wt_b_factors = atom_df0.groupby('residue_number').b_factor.agg('first').values\n",
    "        print(f\"Loaded wildtype b-factors for {len(wt_b_factors)} residues\")\n",
    "    else:\n",
    "        print(\"Wildtype PDB file not found, using placeholder\")\n",
    "        wt_b_factors = np.ones(221) * 80.0  # Placeholder\n",
    "except Exception as e:\n",
    "    print(f\"Error loading wildtype: {e}\")\n",
    "    wt_b_factors = np.ones(221) * 80.0  # Placeholder\n",
    "\n",
    "# Calculate b-factor differences for mutations\n",
    "diffs = []\n",
    "missing_pdbs = []\n",
    "\n",
    "print(\"Processing mutant structures...\")\n",
    "for index, row in test.iterrows():\n",
    "    aa1 = row['wildtype']\n",
    "    aa2 = row['mutation']\n",
    "    pos = row['position']\n",
    "    \n",
    "    # Skip if it's a wildtype sequence or invalid mutation\n",
    "    if aa1 == aa2 or aa1 == 'X':\n",
    "        diffs.append(0.0)\n",
    "        continue\n",
    "    \n",
    "    pdb_filename = f\"{aa1}{pos}{aa2}_unrelaxed_rank_1_model_3.pdb\"\n",
    "    pdb_path = os.path.join(kaggle_dir, pdb_filename)\n",
    "    \n",
    "    try:\n",
    "        if os.path.exists(pdb_path):\n",
    "            atom_df = PandasPdb().read_pdb(pdb_path)\n",
    "            atom_df = atom_df.df['ATOM']\n",
    "            mut_b_factors = atom_df.groupby('residue_number').b_factor.agg('first').values\n",
    "            \n",
    "            # Calculate b-factor difference at mutation position\n",
    "            if pos - 1 < len(mut_b_factors) and pos - 1 < len(wt_b_factors):\n",
    "                d = mut_b_factors[pos - 1] - wt_b_factors[pos - 1]\n",
    "            else:\n",
    "                d = 0.0\n",
    "        else:\n",
    "            d = 0.0\n",
    "            missing_pdbs.append(pdb_filename)\n",
    "    except Exception as e:\n",
    "        d = 0.0\n",
    "        missing_pdbs.append(pdb_filename)\n",
    "    \n",
    "    diffs.append(d)\n",
    "\n",
    "print(f\"Missing PDB files: {len(missing_pdbs)}\")\n",
    "if missing_pdbs:\n",
    "    print(\"Sample missing:\", missing_pdbs[:5])\n",
    "\n",
    "# Create submission\n",
    "sub = pd.read_csv('sample_submission.csv')\n",
    "sub['tm'] = diffs\n",
    "\n",
    "print(\"B-factor difference predictions:\")\n",
    "print(f\"Range: {sub['tm'].min():.3f} to {sub['tm'].max():.3f}\")\n",
    "print(f\"Mean: {sub['tm'].mean():.3f}\")\n",
    "\n",
    "# Save raw b-factor submission\n",
    "sub.to_csv('bfactor.csv', index=False)\n",
    "print(\"Saved bfactor.csv\")\n",
    "\n",
    "# Ensemble with previous best submission (if available)\n",
    "try:\n",
    "    best = pd.read_csv('ensemble_submission.csv')\n",
    "    \n",
    "    # Rank-based ensemble\n",
    "    best_ranks = rankdata(best.tm)\n",
    "    bfactor_ranks = rankdata(sub.tm)\n",
    "    \n",
    "    # Combine ranks (85% best model, 15% b-factor model)\n",
    "    ensemble_ranks = (0.85 * best_ranks + 0.15 * bfactor_ranks) / len(sub)\n",
    "    \n",
    "    submission = sub.copy()\n",
    "    submission.tm = ensemble_ranks\n",
    "    \n",
    "    submission.to_csv('bfactor_thermo.csv', index=False)\n",
    "    print(\"Saved bfactor_thermo.csv (ensemble)\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"ensemble_submission.csv not found, using b-factor only\")\n",
    "    # If no ensemble file, you could use your XGBoost submission instead\n",
    "    try:\n",
    "        xgb_sub = pd.read_csv('submission.csv')\n",
    "        xgb_ranks = rankdata(xgb_sub.tm)\n",
    "        bfactor_ranks = rankdata(sub.tm)\n",
    "        ensemble_ranks = (0.85 * xgb_ranks + 0.15 * bfactor_ranks) / len(sub)\n",
    "        \n",
    "        submission = sub.copy()\n",
    "        submission.tm = ensemble_ranks\n",
    "        submission.to_csv('bfactor_xgb_ensemble.csv', index=False)\n",
    "        print(\"Created ensemble with XGBoost model\")\n",
    "    except:\n",
    "        print(\"Using b-factor predictions only\")\n",
    "\n",
    "final_submission_file = 'bfactor_thermo.csv' if os.path.exists('bfactor_thermo.csv') else 'bfactor.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ThermoNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set the correct paths for your local environment\n",
    "kaggle_dir = os.path.expanduser('~/Desktop/kaggle')\n",
    "\n",
    "# Correct file paths\n",
    "TEST_CSV = os.path.join(kaggle_dir, 'test.csv')\n",
    "WILDTYPE_PDB = os.path.join(kaggle_dir, 'wildtype_structure_prediction_af2.pdb')\n",
    "SAMPLE_SUBMISSION = os.path.join(kaggle_dir, 'sample_submission.csv')\n",
    "\n",
    "print(f\"Looking for files in: {kaggle_dir}\")\n",
    "print(f\"Test CSV path: {TEST_CSV}\")\n",
    "print(f\"Wildtype PDB path: {WILDTYPE_PDB}\")\n",
    "\n",
    "# Check if files exist\n",
    "print(f\"Test CSV exists: {os.path.exists(TEST_CSV)}\")\n",
    "print(f\"Wildtype PDB exists: {os.path.exists(WILDTYPE_PDB)}\")\n",
    "print(f\"Sample submission exists: {os.path.exists(SAMPLE_SUBMISSION)}\")\n",
    "\n",
    "# List files in kaggle directory to see what's available\n",
    "print(\"\\nFiles in kaggle directory:\")\n",
    "for file in os.listdir(kaggle_dir):\n",
    "    if file.endswith('.csv') or file.endswith('.pdb'):\n",
    "        print(f\"  - {file}\")\n",
    "\n",
    "# Load and process test data\n",
    "print(\"\\nLoading test data...\")\n",
    "test_df = pd.read_csv(TEST_CSV)\n",
    "wildtype_seq = \"VPVNPEPDATSVENVALKTGSGDSQSDPIKADLEVKGQSALPFDVDCWAILCKGAPNVLQRVNEKTKNSNRDRSGANKGPFKDPQKWGIKALPPKNPSWSAQDFKSPEEYAFASSLQGGTNAILAPVNLASQNSQGGVLNGFYSANKVAQFDPSKPQQTKGTWFQITKFTGAAGPYCKALGSNDKSVCDKNKNIAGDWGFDPAKWAYQYDEKNNKFNYVGK\"\n",
    "\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(\"Test data columns:\", test_df.columns.tolist())\n",
    "print(\"\\nFirst few rows of test data:\")\n",
    "print(test_df.head())\n",
    "\n",
    "def gen_mutations(name, df, wild):\n",
    "    \"\"\"Simplified mutation detection\"\"\"\n",
    "    result = []\n",
    "    for _, r in df.iterrows():\n",
    "        # Simple mutation detection\n",
    "        found_mutation = False\n",
    "        min_len = min(len(wild), len(r.protein_sequence))\n",
    "        \n",
    "        for i in range(min_len):\n",
    "            if wild[i] != r.protein_sequence[i]:\n",
    "                result.append(['replace', i + 1, wild[i], r.protein_sequence[i]])\n",
    "                found_mutation = True\n",
    "                break\n",
    "        \n",
    "        if not found_mutation:\n",
    "            # Check if it's a length change\n",
    "            if len(r.protein_sequence) != len(wild):\n",
    "                result.append(['length_change', 0, '', ''])\n",
    "            else:\n",
    "                result.append(['same', 0, '', ''])\n",
    "    \n",
    "    df = pd.concat([df, pd.DataFrame(data=result, columns=['op', 'idx', 'wild', 'mutant'])], axis=1)\n",
    "    df['mut'] = df[['wild', 'idx', 'mutant']].astype(str).apply(lambda v: ''.join(v), axis=1)\n",
    "    df['name'] = name\n",
    "    return df\n",
    "\n",
    "# Generate mutation information\n",
    "df_test = gen_mutations('wildtypeA', test_df, wildtype_seq)\n",
    "print(f\"\\nProcessed {len(df_test)} test sequences\")\n",
    "print(\"Mutation analysis:\")\n",
    "print(df_test[['seq_id', 'protein_sequence', 'op', 'idx', 'wild', 'mutant']].head(10))\n",
    "\n",
    "# Since we don't have ThermoNet features, use a simplified structural approach\n",
    "print(\"\\nUsing simplified structural prediction...\")\n",
    "\n",
    "def simple_structural_prediction(row):\n",
    "    \"\"\"Simple stability prediction based on mutation properties\"\"\"\n",
    "    if row['op'] == 'same':\n",
    "        return 0.0  # No change in stability\n",
    "    \n",
    "    if row['op'] == 'length_change':\n",
    "        return -3.0  # Length changes are usually destabilizing\n",
    "    \n",
    "    # For point mutations\n",
    "    wt = row['wild']\n",
    "    mt = row['mutant']\n",
    "    pos = row['idx']\n",
    "    \n",
    "    # Amino acid properties\n",
    "    hydrophobic = ['A', 'V', 'I', 'L', 'M', 'F', 'Y', 'W']\n",
    "    small = ['A', 'G', 'S']\n",
    "    polar = ['N', 'Q', 'S', 'T']\n",
    "    positive = ['R', 'H', 'K']\n",
    "    negative = ['D', 'E']\n",
    "    \n",
    "    score = 0.0\n",
    "    \n",
    "    # Conservative mutations (similar properties) are less disruptive\n",
    "    if (wt in hydrophobic and mt in hydrophobic) or \\\n",
    "       (wt in small and mt in small) or \\\n",
    "       (wt in polar and mt in polar):\n",
    "        score += 0.5  # Slightly stabilizing or neutral\n",
    "    \n",
    "    # Drastic changes are more disruptive\n",
    "    elif (wt in hydrophobic and mt in negative) or \\\n",
    "         (wt in negative and mt in hydrophobic):\n",
    "        score -= 2.5  # Big change\n",
    "    \n",
    "    elif (wt in hydrophobic and mt in positive) or \\\n",
    "         (wt in positive and mt in hydrophobic):\n",
    "        score -= 2.0\n",
    "    \n",
    "    elif (wt in negative and mt in positive) or \\\n",
    "         (wt in positive and mt in negative):\n",
    "        score -= 3.0  # Charge reversal is very disruptive\n",
    "    \n",
    "    else:\n",
    "        score -= 1.0  # Moderate change\n",
    "    \n",
    "    # Adjust based on your model's baseline\n",
    "    return score\n",
    "\n",
    "# Apply predictions\n",
    "df_test['ddg_estimate'] = df_test.apply(simple_structural_prediction, axis=1)\n",
    "\n",
    "# Convert to tm (you'll need to calibrate this)\n",
    "baseline_tm = 55.0  # Adjust based on your training data\n",
    "df_test['tm'] = baseline_tm + df_test['ddg_estimate']\n",
    "\n",
    "print(\"Prediction statistics:\")\n",
    "print(f\"ΔΔG range: {df_test['ddg_estimate'].min():.2f} to {df_test['ddg_estimate'].max():.2f}\")\n",
    "print(f\"tm range: {df_test['tm'].min():.2f} to {df_test['tm'].max():.2f}\")\n",
    "\n",
    "# Create submission\n",
    "submission = pd.read_csv(SAMPLE_SUBMISSION)\n",
    "# Make sure we have the right order\n",
    "submission['tm'] = df_test['tm'].values\n",
    "\n",
    "submission.to_csv('structural_baseline_submission.csv', index=False)\n",
    "print(\"\\nStructural baseline submission saved!\")\n",
    "\n",
    "print(\"Submission preview:\")\n",
    "print(submission.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SASA normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import rankdata\n",
    "import os\n",
    "\n",
    "# Set correct file paths for your environment\n",
    "kaggle_dir = os.path.expanduser('~/Desktop/kaggle')\n",
    "\n",
    "# File paths\n",
    "test_wt_pdb_path = os.path.join(kaggle_dir, \"wildtype_structure_prediction_af2.pdb\")\n",
    "test_csv_path = os.path.join(kaggle_dir, \"test.csv\")\n",
    "new_test_csv_path = os.path.join(kaggle_dir, \"new_test.csv\")  # Make sure this exists\n",
    "sample_submission_path = os.path.join(kaggle_dir, \"sample_submission.csv\")\n",
    "blosum_path = os.path.join(kaggle_dir, \"BLOSUM62_scaled.csv\")  # Make sure this exists\n",
    "\n",
    "print(\"Checking file existence:\")\n",
    "print(f\"Wildtype PDB: {os.path.exists(test_wt_pdb_path)}\")\n",
    "print(f\"Test CSV: {os.path.exists(test_csv_path)}\")\n",
    "print(f\"New Test CSV: {os.path.exists(new_test_csv_path)}\")\n",
    "print(f\"Sample Submission: {os.path.exists(sample_submission_path)}\")\n",
    "print(f\"BLOSUM matrix: {os.path.exists(blosum_path)}\")\n",
    "\n",
    "# Load data\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "\n",
    "# Check if new_test.csv exists before merging\n",
    "if os.path.exists(new_test_csv_path):\n",
    "    test_df_w_extra_info = pd.read_csv(new_test_csv_path)\n",
    "    test_df = test_df.merge(\n",
    "        test_df_w_extra_info[[\"seq_id\", \"edit_type\", \"edit_idx\", \"wildtype_aa\", \"mutant_aa\"]], \n",
    "        on=\"seq_id\", how=\"left\"\n",
    "    ).fillna(-1)\n",
    "    test_df['edit_idx'] = test_df['edit_idx'].astype(int)\n",
    "else:\n",
    "    print(\"new_test.csv not found, creating placeholder columns\")\n",
    "    test_df['edit_type'] = 'unknown'\n",
    "    test_df['edit_idx'] = -1\n",
    "    test_df['wildtype_aa'] = ''\n",
    "    test_df['mutant_aa'] = ''\n",
    "\n",
    "ss_df = pd.read_csv(sample_submission_path)\n",
    "\n",
    "# Load BLOSUM matrix if available\n",
    "if os.path.exists(blosum_path):\n",
    "    blosum = pd.read_csv(blosum_path)\n",
    "    blosum.set_index('amino-acids', inplace=True)\n",
    "else:\n",
    "    print(\"BLOSUM matrix not found, creating placeholder\")\n",
    "    # Create a simple BLOSUM-like matrix\n",
    "    amino_acids = list('ACDEFGHIKLMNPQRSTVWY')\n",
    "    blosum = pd.DataFrame(1.0, index=amino_acids, columns=amino_acids)\n",
    "    np.fill_diagonal(blosum.values, 2.0)  # Higher score for identical residues\n",
    "\n",
    "wildtype_aa = \"VPVNPEPDATSVENVALKTGSGDSQSDPIKADLEVKGQSALPFDVDCWAILCKGAPNVLQRVNEKTKNSNRDRSGANKGPFKDPQKWGIKALPPKNPSWSAQDFKSPEEYAFASSLQGGTNAILAPVNLASQNSQGGVLNGFYSANKVAQFDPSKPQQTKGTWFQITKFTGAAGPYCKALGSNDKSVCDKNKNIAGDWGFDPAKWAYQYDEKNNKFNYVGK\"\n",
    "\n",
    "print(\"Test data with mutation info:\")\n",
    "print(test_df.head())\n",
    "\n",
    "# Calculate SASA features\n",
    "from Bio.PDB import PDBParser, ShrakeRupley\n",
    "\n",
    "pdb_parser = PDBParser(QUIET=1)\n",
    "\n",
    "def get_sr_sasa(pdb_path, return_struct=False, pdb_identifier=\"AF70\", sr_n_points=250):\n",
    "    try:\n",
    "        sr = ShrakeRupley(n_points=sr_n_points)\n",
    "        struct = pdb_parser.get_structure(pdb_identifier, pdb_path)\n",
    "        sr.compute(struct, level=\"R\")\n",
    "        if return_struct:\n",
    "            return struct\n",
    "        else:\n",
    "            return [x.sasa for x in struct.get_residues()]\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating SASA: {e}\")\n",
    "        # Return placeholder values\n",
    "        return [0.0] * 221  # Wildtype length\n",
    "\n",
    "if os.path.exists(test_wt_pdb_path):\n",
    "    sasa_by_residue = get_sr_sasa(test_wt_pdb_path)\n",
    "    residue_idx_to_sasa = {i: _sasa for i, _sasa in enumerate(sasa_by_residue)}\n",
    "    test_df[\"sasa\"] = test_df[\"edit_idx\"].apply(lambda x: residue_idx_to_sasa.get(x, 0.0))\n",
    "else:\n",
    "    print(\"Wildtype PDB not found, using placeholder SASA values\")\n",
    "    test_df[\"sasa\"] = 100.0  # Placeholder\n",
    "\n",
    "print(\"Test data with SASA:\")\n",
    "print(test_df.head(10))\n",
    "\n",
    "# Multiply SASA values by BLOSUM substitution scores\n",
    "for k in range(test_df.shape[0]):\n",
    "    if test_df['edit_type'].iloc[k] == 'substitution':\n",
    "        wt_aa = test_df['wildtype_aa'].iloc[k]\n",
    "        mt_aa = test_df['mutant_aa'].iloc[k]\n",
    "        if wt_aa in blosum.index and mt_aa in blosum.columns:\n",
    "            blosum_score = blosum.loc[wt_aa, mt_aa]\n",
    "            test_df.loc[test_df.index[k], 'sasa'] = test_df['sasa'].iloc[k] * blosum_score\n",
    "\n",
    "# Set deletions to the lowest score at each position\n",
    "for k in range(test_df.shape[0]):\n",
    "    if test_df['edit_type'].iloc[k] == 'deletion':\n",
    "        edit_idx = test_df['edit_idx'].iloc[k]\n",
    "        same_position_sasa = test_df[test_df['edit_idx'] == edit_idx]['sasa']\n",
    "        if len(same_position_sasa) > 0:\n",
    "            test_df.loc[test_df.index[k], 'sasa'] = same_position_sasa.min()\n",
    "\n",
    "print(\"Test data with adjusted SASA:\")\n",
    "print(test_df.head())\n",
    "\n",
    "# Create SASA-based submission\n",
    "ss_df_sasa = ss_df.copy()\n",
    "ss_df_sasa[\"tm\"] = test_df[\"sasa\"].values\n",
    "ss_df_sasa.to_csv(\"pure_chem_sasa_residual_v1.csv\", index=False)\n",
    "print(\"SASA-based submission saved!\")\n",
    "\n",
    "# Load other submission files for ensemble\n",
    "print(\"Loading ensemble components...\")\n",
    "\n",
    "# SASA component\n",
    "sasa_r = pd.read_csv('pure_chem_sasa_residual_v1.csv')\n",
    "\n",
    "# Try to load other submission files with error handling\n",
    "try:\n",
    "    # Replace with your actual submission files\n",
    "    sub = pd.read_csv(os.path.join(kaggle_dir, \"bfactor.csv\"))  # b-factor submission\n",
    "except:\n",
    "    print(\"bfactor.csv not found, using SASA only\")\n",
    "    sub = sasa_r.copy()\n",
    "\n",
    "try:\n",
    "    best = pd.read_csv(os.path.join(kaggle_dir, \"submission.csv\"))  # best submission\n",
    "except:\n",
    "    print(\"Best submission not found, using SASA\")\n",
    "    best = sasa_r.copy()\n",
    "\n",
    "try:\n",
    "    v17 = pd.read_csv(os.path.join(kaggle_dir, \"submission_ver17.csv\"))\n",
    "except:\n",
    "    print(\"v17 submission not found, skipping\")\n",
    "    v17 = sasa_r.copy()\n",
    "\n",
    "# Create ensemble using rank-based weighting\n",
    "print(\"Creating ensemble...\")\n",
    "submission_ensemble = ss_df.copy()\n",
    "\n",
    "# Use rank-based ensemble (more robust than raw values)\n",
    "ensemble_tm = (\n",
    "    0.15 * rankdata(sub.tm) + \n",
    "    0.78 * rankdata(best.tm) + \n",
    "    0.00 * rankdata(sasa_r.tm) +  # Weight set to 0 as in original\n",
    "    0.07 * rankdata(v17.tm)\n",
    ") / len(submission_ensemble)\n",
    "\n",
    "submission_ensemble[\"tm\"] = ensemble_tm\n",
    "submission_ensemble.to_csv('bfactor_sasa-R_energy-score_ensemble.csv', index=False)\n",
    "\n",
    "print(\"Ensemble submission preview:\")\n",
    "print(submission_ensemble.head())\n",
    "\n",
    "# Also create a simpler version\n",
    "simple_ensemble = ss_df.copy()\n",
    "simple_ensemble[\"tm\"] = (0.8 * rankdata(best.tm) + 0.2 * rankdata(sasa_r.tm)) / len(simple_ensemble)\n",
    "simple_ensemble.to_csv('simple_ensemble.csv', index=False)\n",
    "\n",
    "print(\"Simple ensemble preview:\")\n",
    "print(simple_ensemble.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
